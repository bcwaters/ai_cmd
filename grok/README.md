# grok_cmd
query grok API and generate a simple readme in the browser. context the prior conversations is sent along each message.


## how to run
first clone this repository 

next run ```npm install```

now to query grok run ```./grok.sh "your prompt" ```



## future features

TODO debug markdown parsing output. it seem ```<script> tag break it and ``` ticks ```

!important: the markdown parser will be replaced later. The priniciple remains the same. Serve pure html generated by readmes and templates on the backend.

  Template html needs to be improved for tree mode.  THere should be a parent home link that can be clicked to return to the home page.
     When a page is marked as parent headings should be linked to their children.

  After the first prompt generate a list of suggesting prompts that can be clicked to load into the prompt.

  save a webpage to browser cahce for the user, or upcharge for hosting it for them... these are so small i wonder if a db can safely save 100kb html files  

  recieve a current direcrty option to decide where docs are saved.

  save pure readme files before processing

  create a flag for verbose logging and clean logging

  improve the history storage approach

  send as a facebook message from html
  

  create a docker image to allow others to try this out easier
 
  more robust cmd options
  ! BIG IDEA ! the runtime can set all the arguments and the js script loads them from the .grokRuntime file. this make the argument parsing so much easier.

  Profile prompt improvements  
      -- always site sources  (When provide information add a link to source the information)
      -- language tutor (From time to time add a note on how to say it in a different language)

  Organize  directory structure for history and context

  Refactor helper functions in grok.js to be more useful, I may pass around a state class to all functions to reference.

tree mode can be more intelligent.  
   * increase depth when list has nested lists
   * sticky mode.  the context is sticky to the prompt.  the context is not updated for the next prompt.
    
  * update mode.  appends additional profile user prompts
  * dynamic context setting.
  * set root context for quick switching
  * an example would be branch: generate a readme doc for the prompt but do not update the context for the next prompt

  * when displaying the context id display the parent context and any current children

  * a file option to load a file form the specified path the user givesj
    image mode to switch models
    a flag to switch the model

    more... type more to get a full list of options

  
  Encapulate system prompt in a file and load it.
     allow for different profiles

  Further refinement on grok response output.

  Paste files into prompt, or refer to them for js to load into the prompt. can i access vim while grok is running?

  output the directory tree structure of the current context in relation to the prompt history

  add marked css for a modern look of the markdown

  implement a crawler that has options to prompt for more info on the heading. simply click and type. the markdown will be the way to create the file structure.

  cursor rule for debugging: That fixed it.  The jpeg is encoding. If possible can you generate a readme doc which gives an overview of our troubleshotting process for casting uint8

  improve the context file structure to be more robust. KEYWORDS HEADINGS PROMPTS USESRPROFILE 


  CLAUDE CODE NOTES
  I like the usage of background colors to make the starting text
  I also like the colored '-' for borders

  How is the terminal breaker line dynamic to window size

  It would be nice to output results to different mediumns.
      - slack: runs slack.sh to output resuts to a chat
      - facebook: runs facebook.sh to output results to a facebook post or message

Local LLM values
    It would be nice to replace my context files with a call to a local LLM.  
      The flow would be:     
       * user prompts server.  
       * Entire prompt including context I've built up is sent to the local LLM.  MANY MANY MANY Tokens  
       * The LLM would then process the tokens and return a semantically accurate list of words.  
       * the prompt is AUGMENTED with the semantic list and prompt question  
       * the augmented prompt is sent to OPENAI  
       * process response as usual (chance to finetune llm?)  
       * repeat  
Planned architecture:
   NGINX - serve static html files from prompts
   VECTOR STORE - serves to cache popular prompts and prime the result with compressed semantic context
   nodejs(replace with bun) - javascript runtime for grok.js
   OPENAI - Outside my control unless I host an LLM


I like the spinner and countdown timer

  How do they login with a shell cript to browser? is it polling an enpoint to check for confirmation?

 ```sudo chown -R $USER:$(id -gn) /usr/local && sudo chmod -R u+w /usr/local ``` why?

 Does claude charge me for images in my repo? is there a .claudeignore to reduce costs?
 