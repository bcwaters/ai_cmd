# Softmax Function

The **softmax function**, also known as the normalized exponential function, is primarily used in machine learning, especially in neural networks for multi-class classification problems. It converts a vector of \( K \) real numbers into a probability distribution of \( K \) possible outcomes.

## How Softmax Works

Given an input vector \( \mathbf{z} \) of \( K \) real numbers, the softmax function calculates the probability \( \sigma(\mathbf{z})_i \) for each component \( z_i \) as follows:

\[ \sigma(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}} \]

Here's a step-by-step breakdown of the softmax calculation:

1. **Exponentiation**: Each element \( z_i \) in the input vector \( \mathbf{z} \) is exponentiated, i.e., \( e^{z_i} \). This ensures all values are positive.

2. **Normalization**: The exponentiated values are then normalized by dividing each by the sum of all exponentiated values, \( \sum_{j=1}^K e^{z_j} \). This ensures that the resulting values sum to 1, forming a valid probability distribution.

### Example

Consider a vector \( \mathbf{z} = [1.0, 2.0, 3.0] \):

1. Calculate \( e^{z_i} \):
   - \( e^{1.0} \approx 2.718 \)
   - \( e^{2.0} \approx 7.389 \)
   - \( e^{3.0} \approx 20.085 \)

2. Sum these values: 
   - \( 2.718 + 7.389 + 20.085 = 30.192 \)

3. Normalize by dividing each exponentiated value by the sum:
   - \( \sigma(\mathbf{z})_1 = \frac{2.718}{30.192} \approx 0.090 \)
   - \( \sigma(\mathbf{z})_2 = \frac{7.389}{30.192} \approx 0.245 \)
   - \( \sigma(\mathbf{z})_3 = \frac{20.085}{30.192} \approx 0.665 \)

The resulting vector \( \sigma(\mathbf{z}) \approx [0.090, 0.245, 0.665] \) is a probability distribution where the probabilities sum to 1.

## Use in Neural Networks

In neural networks, the softmax function is typically applied to the output layer when the task is multi-class classification. It transforms the raw output scores (logits) into a probability distribution over the predicted output classes.

### Advantages

- **Interpretability**: Outputs are interpretable as probabilities.
- **Differentiability**: The softmax function is differentiable, making it suitable for use with gradient-based optimization methods like backpropagation.

### Limitations

- **Numerical Stability**: Softmax can suffer from numerical instability due to the exponential function, especially with very large input values. Techniques such as subtracting the maximum value from the input vector before exponentiation can mitigate this issue.

Understanding the softmax function is crucial for anyone working with machine learning models, particularly those involved in classification tasks.

