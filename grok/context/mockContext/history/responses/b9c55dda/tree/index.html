<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>Home Page</title>
</head>
<body>
  <nav>
    <button id="saveButton" onclick="saveButton()">Save html</button>
    <a id="parentContent" href='javascript:goHomePage()'>Home</a>
    <select id="childSubjects">
      <option value="home">Home</option>
      <!-- Children are added here onload with setChildSubjects -->
    </select>
    <!-- Add a previous and next button for prior requests? -->
    <!-- Add a button to save the current page as a markdown file -->
  </nav>

  <div id="content">  
    <div id="parentContent">
        <h2>Embedding Generation</h2>
<p><strong>Embedding Generation</strong> is the process of converting preprocessed text into vector embeddings using a model suited for semantic understanding. Key steps include:</p>
<ul>
<li><strong>Model Selection</strong>: Choose a model like BERT or other transformer-based models that excel at capturing semantic relationships. These models can be fine-tuned on your specific dataset if necessary.</li>
<li><strong>Embedding Use</strong>: Use the selected model to generate embeddings for each entry. These embeddings should capture the semantic essence of the text.</li>
</ul>
<h2>Data Preprocessing</h2>
<p><strong>Data Preprocessing</strong> is crucial for preparing your data to be loaded into a vector database, enhancing semantic understanding. Key steps include:</p>
<ul>
<li><strong>Text Preprocessing</strong>: Apply techniques such as lowercasing, removing special characters, punctuation, and numbers, as well as handling stopwords, stemming, lemmatization, and negation handling. This reduces noise and focuses on the semantic content of the data.</li>
<li><strong>Standardization</strong>: Ensure that your data is in a consistent format to improve the model's ability to generalize and process it accurately.</li>
</ul>
<p>By following these steps, you can effectively prepare your data for semantic search and retrieval using a vector database.</p>

    </div>
    <div id="childDivs">
      <div title=""Embedding Generation" id="childContent1" onclick="setVisibileChild('childContent1')" hidden=true><h1>Embedding Generation</h1>
<p>Embedding generation is a critical process in the realm of semantic search and retrieval, particularly when using vector databases. This process involves converting text data into numerical vectors, known as embeddings, which capture the semantic essence of the text. Here's a detailed look into the aspects of embedding generation that were previously mentioned:</p>
<h2>Data Preprocessing</h2>
<p>Before generating embeddings, data must be preprocessed to ensure it is clean and suitable for the embedding model. This involves several steps:</p>
<ul>
<li><p><strong>Lowercasing</strong>: Converting all text to lowercase ensures uniformity and helps in reducing the vocabulary size. This step is crucial as it treats words like "Hello" and "hello" the same, which is important for semantic analysis.</p>
</li>
<li><p><strong>Removing Special Characters, Punctuation, and Numbers</strong>: These elements can often be irrelevant to the semantic meaning of the text. Removing them helps in focusing on the actual content. For example, "hello!" becomes "hello".</p>
</li>
<li><p><strong>Handling Stopwords</strong>: Stopwords are common words that carry minimal semantic value, e.g., "is", "the". Removing them can reduce the dimensionality of the data and improve focus on more meaningful terms.</p>
</li>
<li><p><strong>Stemming and Lemmatization</strong>: These techniques reduce words to their root forms. Stemming cuts off the ends of words, e.g., "running" to "run", while lemmatization uses a vocabulary and morphologic analysis to return the base or dictionary form of a word, e.g., "better" to "good". Both methods help in reducing vocabulary size and improving generalization.</p>
</li>
<li><p><strong>Negation Handling</strong>: Properly handling negation is crucial for maintaining semantic integrity. For example, "not good" should be treated differently from "good".</p>
</li>
</ul>
<h2>Model Selection</h2>
<p>Choosing the right model for generating embeddings is crucial. Popular choices include:</p>
<ul>
<li><strong>BERT and Transformers</strong>: These models are highly effective in capturing context and semantic relationships in text. BERT, in particular, can generate high-quality embeddings that are useful for a wide range of applications.</li>
</ul>
<h2>Embedding Generation Process</h2>
<p>Once the data is preprocessed and the model is selected, the embedding generation process involves:</p>
<ol>
<li><strong>Tokenization</strong>: Breaking down the text into tokens that the model can process.</li>
<li><strong>Encoding</strong>: Converting these tokens into numerical representations that the model understands.</li>
<li><strong>Embedding</strong>: Using the model to generate dense vector representations of the text, which encapsulate its semantic meaning.</li>
</ol>
<p>These embeddings are then stored in a vector database, allowing for efficient similarity searches and semantic retrieval. The quality of the embeddings directly impacts the performance of semantic search and retrieval systems.</p>
<p>By following these steps, you can effectively prepare your data for semantic search and retrieval using vector databases. Proper data preprocessing not only enhances the performance of your models but also ensures that the semantic essence of the text is captured accurately.</p>
<p>ResponseID:83327541</p>
</div><div title=" Data Preprocessing"" id="childContent2" onclick="setVisibileChild('childContent2')" hidden=true><h1>Data Preprocessing</h1>
<p>Data preprocessing is a critical step in preparing your data before it is loaded into a vector database and used for enhancing semantic understanding. The goal of data preprocessing is to transform raw data into a clean and usable format that can be effectively processed by machine learning models. Here are the key steps involved in data preprocessing:</p>
<h2>Text Preprocessing</h2>
<p>Text preprocessing involves several techniques to clean and normalize text data. This step is essential to reduce noise and focus on the semantic content of the text. Key techniques include:</p>
<ul>
<li><p><strong>Lowercasing:</strong> Converting all text to lowercase ensures uniformity and helps in reducing the vocabulary size. This step is crucial as it treats words like "Hello" and "hello" as the same, which is important for semantic analysis.</p>
</li>
<li><p><strong>Removing Special Characters, Punctuation, and Numbers:</strong> These elements can often be irrelevant to the semantic meaning of the text. Removing them helps in focusing on the actual content. For example, "hello!" becomes "hello".</p>
</li>
<li><p><strong>Handling Stopwords:</strong> Stopwords are common words that carry minimal semantic value (e.g., "the", "is", "at"). Removing them can reduce the dimensionality of the data and improve the focus on more meaningful terms.</p>
</li>
<li><p><strong>Stemming and Lemmatization:</strong> These techniques reduce words to their root forms. Stemming cuts off the ends of words (e.g., "running" to "run"), while lemmatization uses vocabulary and morphological analysis to return the base or dictionary form of a word (e.g., "better" to "good"). Both methods help in reducing the vocabulary size and improving generalization.</p>
</li>
<li><p><strong>Negation Handling:</strong> Properly handling negations is crucial for maintaining the semantic integrity of the text. For example, "not good" should be treated differently from "good".</p>
</li>
</ul>
<h2>Data Standardization</h2>
<p>Data standardization ensures that the data is consistent across the dataset, which is crucial for improving the model's ability to generalize and process the data accurately. Key aspects include:</p>
<ul>
<li><p><strong>Consistency in Analysis:</strong> Ensuring that similar data entries are treated the same way across the dataset. For example, dates should be formatted consistently.</p>
</li>
<li><p><strong>Language Specifics and Acronym Handling:</strong> Addressing language-specific rules and properly handling acronyms to ensure that the data remains meaningful and interpretable.</p>
</li>
</ul>
<p>By following these steps, you can effectively prepare your data for semantic search and retrieval using a vector database. Proper data preprocessing not only enhances the performance of your models but also ensures that the semantic essence of the text is captured accurately.</p>
<p>ResponseID:947c6fcb</p>
</div>
    </div>
  </div>

    <script>
        function saveButton() {
          const content = document.documentElement.outerHTML; // Get the entire HTML content
          const blob = new Blob([content], { type: 'text/html' }); // Create a Blob from the content
          const url = URL.createObjectURL(blob); // Create a URL for the Blob
          const divInnerText = document.getElementById('content').innerText;
          let filename = divInnerText.substring(0, 25);
          const a = document.createElement('a'); // Create an anchor element
          a.href = url; // Set the href to the Blob URL
          
        a.download =  filename + '.html'; // Set the download attribute with a filename
          document.body.appendChild(a); // Append the anchor to the body
          a.click(); // Programmatically click the anchor to trigger the download
          document.body.removeChild(a); // Remove the anchor from the document
          URL.revokeObjectURL(url); // Release the Blob URL
        }
    </script>

    <script>
      function setChildSubjects(){
        let dropDownOptions = [];
        let childSubjects = document.getElementById('childDivs');
        for (let child of childSubjects.children){
           let subject = child.title;
           let optionValue = child.id;

           dropDownOptions.push({subject: subject, value: optionValue});
        
        }

        for (let option of dropDownOptions){
          let optionElement = document.createElement('option');
          optionElement.value = option.value;
          optionElement.text = option.subject;
          document.getElementById('childSubjects').appendChild(optionElement);
        }

        document.getElementById('childSubjects').addEventListener('change', function() {
          if(this.value == "home"){
            goHomePage();
          }else{
            setVisibileChild(this.value);
          }

          }
        );
      }
    </script>


    <script>
      function setVisibileChild(id){
        //quick flicker home to reset state, this allows hoping between child views
         goHomePage();
         if(id != "parentContent"){
           let topNode = document.getElementById('content');
           let parentNode = document.getElementById('parentContent');
           let childVisibleNode = document.getElementById(id);
           parentNode.hidden = true;
           childVisibleNode.hidden = false;
         }
      }
    </script>

    <script>
      function goHomePage(){
         let topNode = document.getElementById('content');
         let parentNode = document.getElementById('parentContent');
         let children = document.getElementById('childDivs');
         for (let child of children.children){
            child.hidden = true;
         }
         parentNode.hidden = false;
      }
    </script>

    <script>
      function setChildLinks(){
        let children = document.getElementById('childDivs'); 
        let childSubjects = [];
        let discoveredMatches = [];

        
        for (let child of children.children){
          let subject = child.title;
          childSubjects.push({subject: subject, child: child});
          }
             
          let parentNodeH2Subjects = []
          let parentNodeH3Subjects = []
          let parentNodeH4Subjects = []
          let parentNode = document.getElementById('parentContent');
          let H2s = parentNode.getElementsByTagName("H2");
          let H3s = parentNode.getElementsByTagName("H3");
          let H4s = parentNode.getElementsByTagName("H4");
          let isH2Match = false;
          let isH3Match = false;
          let isH4Match = false;

          H2s.length > 0? parentNodeH2Subjects = H2s.map(item => ({subject: item.innerText, item: item})):isH2Match = false;
          H3s.length > 0?parentNodeH3Subjects = H3s.map(item => ({subject: item.innerText, item: item})):isH3Match = false;
          H4s.length > 0?parentNodeH4Subjects = H4s.map(item => ({subject: item.innerText, item: item})):isH4Match = false;

          
           isH3Match = H3s.length == childSubjects.length;
           isH2Match = H2s.length == childSubjects.length;
           isH4Match = H4s.length == childSubjects.length;
          let allDiscovered = false;

          if (isH3Match){
            //We have a match.  We need to find the H2s
            allDiscovered = true;
          }
          if (isH2Match){
            //We have a match.  We need to find the H3s
            allDiscovered = true;
          }
          if (isH4Match){
            //We have a match.  We need to find the H2s
            allDiscovered = true;
          }

          if(allDiscovered && (isH3Match + isH2Match + isH4Match) > 1){
            //wierd.  ok we work from scratch.
            allDiscovered = false;
          }

          if(!allDiscovered){

            let fluffWords = ["a", "an", "the", "and", "but", "or", "for", "nor", "on", "at", "to", "from", "by", "with", "of"];
            let fluffWordsRegex = new RegExp(fluffWords.join("|"), "g");
          
          //OK time to stumble through unpredictable llm output
          //First check if the child subjects are in the parent node h2 subjects
          let isH2 = false;
          let isH3 = false;
          let isH4 = false;
      
          //This can be optimized later.  Probably doesnt matter since it is client side with modern computing.
          for (let i = 0; i < parentNodeH2Subjects.length; i++){
            let subject = parentNodeH2Subjects[i].subject;
            for (let j = 0; j < childSubjects.length; j++){
              let childSubject = childSubjects[j].subject;
              if(childSubject.replace(fluffWordsRegex, "") == subject.replace(fluffWordsRegex, "")){
                isH2 = true;
                discoveredMatches.push({parentLink: parentNodeH2Subjects[i].item, childLink: childSubjects[j].child});
              }
            }

          }
         
          if(!isH2){
            for (let i = 0; i < parentNodeH3Subjects.length; i++){
              let subject = parentNodeH3Subjects[i].subject;
              for (let j = 0; j < childSubjects.length; j++){
                let childSubject = childSubjects[j].subject;  
                if(childSubject.replace(fluffWordsRegex, "") == subject.replace(fluffWordsRegex, "")){
                  isH3 = true;
                  discoveredMatches.push({parentLink: parentNodeH3Subjects[i].item, childLink: childSubjects[j].child});
                }
              }
            }
          }

          if(!isH3){
            for (let i = 0; i < parentNodeH4Subjects.length; i++){
              let subject = parentNodeH4Subjects[i].subject;
              for (let j = 0; j < childSubjects.length; j++){
                let childSubject = childSubjects[j].subject;
                if(childSubject.replace(fluffWordsRegex, "") == subject.replace(fluffWordsRegex, "")){
                  isH4 = true;
                  discoveredMatches.push({parentLink: parentNodeH4Subjects[i].item, childLink: childSubjects[j].child});
                }
              }
            }
          }

          if(!isH4){
              //I suppose there are edge cases but this surely covers 99.9% of the cases.
          }

          }else{
            if(isH3Match){
              for (let i = 0; i < parentNodeH3Subjects.length; i++){
                let child = childSubjects[i].child;
                makeChildLinks(parentNodeH3Subjects[i].item, child);
              }
              return; //bye bye
            }
            if(isH2Match){
              for (let i = 0; i < parentNodeH2Subjects.length; i++){
                let child = childSubjects[i].child;
                makeChildLinks(parentNodeH2Subjects[i].item, child);
                return; //bye bye
              }
              if(isH4Match){
                for (let i = 0; i < parentNodeH4Subjects.length; i++){
                  let child = childSubjects[i].child;
                  makeChildLinks(parentNodeH4Subjects[i].item, child);
                }
              }

            }

          }

          for (let match of discoveredMatches){
            makeChildLinks(match.parentLink, match.childLink);
            if(match.length < childSubjects.length){ alert("LLMM added additional info that is hidden");}
          }

        }
    </script>

    <script>
       function makeChildLinks(parent, child){
            parent.style.cursor = "pointer";
            parent.style.textDecoration = "underline";
            parent.style.color = "blue";
            parent.onclick = function(){
              setVisibileChild(child.id);
            }
       }

    </script>

<script>window.onload = setChildSubjects;</script>

</body>
</html>


   
    <!-- <textarea id="userPrompt"></textarea>

    <script>
        function nextButton() {
            const userPrompt = document.getElementById('userPrompt').value;
            console.log(userPrompt);
        }
    </script> -->