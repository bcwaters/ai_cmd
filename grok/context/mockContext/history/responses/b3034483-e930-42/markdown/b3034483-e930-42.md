# Decision Tree Regressor Algorithm Walkthrough

Here's a simplified walkthrough of how a Decision Tree Regressor operates, using ASCII art for better visualization:

```
       Start
        |
        v
   Select Best Split
        |
        v
   [Is Split Better?]
   /           \
  No           Yes
  /             \
  /               \
  v                 v
Leaf Node        Split Node
  (Predict)      /       \
                /         \
               v           v
            Left Subtree  Right Subtree
```

### Explanation:

1. **Start**: The algorithm begins at the root node.

2. **Select Best Split**: At each node, the algorithm searches for the best feature and threshold to split the data. This is done by evaluating a criterion, commonly Mean Squared Error (MSE) for regression.

3. **Is Split Better?**: If the split reduces the MSE (or another chosen metric), the algorithm proceeds to create a split node.

4. **Leaf Node**: If no beneficial split is found, the node becomes a leaf node, and it will predict the average (or another central tendency measure) of the target values of the instances that reach this node.

5. **Split Node**: If a beneficial split is found, the node splits into two child nodes (left and right subtrees), and the process repeats for these subtrees.

6. **Left Subtree and Right Subtree**: The data is divided based on the split condition, and the algorithm continues recursively until it reaches the stopping criteria (e.g., maximum depth, minimum samples per leaf).

This process constructs a tree structure where each internal node represents a decision based on a feature, and each leaf node contains a prediction value.

