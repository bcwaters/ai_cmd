<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>Home Page</title>
  <style>
    pre code.hljs{display:block;overflow-x:auto;padding:1em}code.hljs{padding:3px 5px}/*!
  Theme: StackOverflow Light
  Description: Light theme as used on stackoverflow.com
  Author: stackoverflow.com
  Maintainer: @Hirse
  Website: https://github.com/StackExchange/Stacks
  License: MIT
  Updated: 2021-05-15

  Updated for @stackoverflow/stacks v0.64.0
  Code Blocks: /blob/v0.64.0/lib/css/components/_stacks-code-blocks.less
  Colors: /blob/v0.64.0/lib/css/exports/_stacks-constants-colors.less
*/.hljs{color:#2f3337;background:#f6f6f6}.hljs-subst{color:#2f3337}.hljs-comment{color:#656e77}.hljs-attr,.hljs-doctag,.hljs-keyword,.hljs-meta .hljs-keyword,.hljs-section,.hljs-selector-tag{color:#015692}.hljs-attribute{color:#803378}.hljs-name,.hljs-number,.hljs-quote,.hljs-selector-id,.hljs-template-tag,.hljs-type{color:#b75501}.hljs-selector-class{color:#015692}.hljs-link,.hljs-regexp,.hljs-selector-attr,.hljs-string,.hljs-symbol,.hljs-template-variable,.hljs-variable{color:#54790d}.hljs-meta,.hljs-selector-pseudo{color:#015692}.hljs-built_in,.hljs-literal,.hljs-title{color:#b75501}.hljs-bullet,.hljs-code{color:#535a60}.hljs-meta .hljs-string{color:#54790d}.hljs-deletion{color:#c02d2e}.hljs-addition{color:#2f6f44}.hljs-emphasis{font-style:italic}.hljs-strong{font-weight:700}

    nav {
      display: flex;
      justify-content: space-between;
      align-items: left;
      padding: 10px;
    }
    
    .nav-right {
      display: flex;
      gap: 50px;
      align-items: center;
    }
  </style>
</head>
<body>
  <nav>
    <a id="parentContentLink" href='/?context=a4ffabfb-1b8a-41'>Prompt again</a>

      <select id="childSubjects">
        <option value="home">Home</option>
        <!-- Children are added here onload with setChildSubjects -->
      </select>
      <!-- Add a previous and next button for prior requests? -->
      <!-- Add a button to save the current page as a markdown file -->

      
      <button id="saveButton" onclick="saveButton()">Save html</button>


  </nav>

  <div id="content">  
    <div id="parentContent">
        <p>Here are two common equations used for stochastic evaluation in neural networks:</p>
<h3>1. Stochastic Gradient Descent (SGD) Update Rule</h3>
<p>The update rule for weights in a neural network using Stochastic Gradient Descent can be expressed as:</p>
<p>[ w_{new} = w_{old} - \eta \cdot 
abla J(w_{old}) ]</p>
<p>Where:</p>
<ul>
<li>( w_{new} ) is the updated weight.</li>
<li>( w_{old} ) is the current weight.</li>
<li>( \eta ) is the learning rate.</li>
<li>( 
abla J(w_{old}) ) is the gradient of the loss function with respect to the weight.</li>
</ul>
<h3>2. Mini-batch Gradient Descent</h3>
<p>Mini-batch gradient descent is a variation of SGD where the gradient is computed on a small subset of the data. The update rule is similar but calculated over a mini-batch:</p>
<p>[ w_{new} = w_{old} - \eta \cdot \frac{1}{m} \sum_{i=1}^{m} 
abla J(w_{old}, x_i, y_i) ]</p>
<p>Where:</p>
<ul>
<li>( m ) is the size of the mini-batch.</li>
<li>( 
abla J(w_{old}, x_i, y_i) ) is the gradient of the loss function with respect to the weight, calculated for each example ( (x_i, y_i) ) in the mini-batch.</li>
</ul>
<p>Would you like to know more about how these equations are applied in practice or about other stochastic methods in neural networks?</p>

    </div>
    <div id="childDivs">
      <div title="Stochastic Gradient Descent" id="childContent1" onclick="setVisibileChild('childContent1')" hidden=true><h1>Stochastic Gradient Descent (SGD) Update Rule in Detail</h1>
<h2>Overview</h2>
<p>Stochastic Gradient Descent (SGD) is an optimization algorithm used to minimize a loss function in machine learning models, particularly in neural networks. It's called "stochastic" because it uses a single training example (chosen randomly) to perform each iteration of the gradient descent. This randomness introduces noise into the process, which can help escape local minima in the loss landscape.</p>
<h2>The Update Rule Equation</h2>
<p>The SGD update rule for updating weights in a neural network is given by:</p>
<p>[ w_{new} = w_{old} - \eta \cdot 
abla J(w_{old}) ]</p>
<p>Where:</p>
<ul>
<li>( w_{new} ) represents the new weight after the update.</li>
<li>( w_{old} ) is the current weight before the update.</li>
<li>( \eta ) (eta) is the learning rate, a hyperparameter that controls how much to adjust the weights with respect to the loss gradient.</li>
<li>( 
abla J(w_{old}) ) is the gradient of the loss function ( J ) with respect to the weight ( w_{old} ).</li>
</ul>
<h2>Detailed Breakdown</h2>
<h3>Weights (( w ))</h3>
<p>Weights in a neural network are the parameters that the model learns during training. They are adjusted to minimize the error between the predicted output and the actual output. In SGD, weights are updated one at a time based on the gradient calculated from a single training example.</p>
<h3>Learning Rate (( \eta ))</h3>
<p>The learning rate is a crucial hyperparameter that determines the step size at each iteration while moving toward a minimum of the loss function. If the learning rate is too high, the algorithm might overshoot the minimum; if it's too low, the training process might be too slow or get stuck in a local minimum.</p>
<h3>Gradient ((</h3>
<p>abla J(w_{old}) ))</p>
<p>The gradient is a vector of partial derivatives of the loss function with respect to the weights. It points in the direction of the steepest increase in the loss function. By moving in the opposite direction (subtracting the gradient from the current weight), SGD aims to minimize the loss function.</p>
<h3>Loss Function (( J ))</h3>
<p>The loss function quantifies the error between the predicted and actual values. Common loss functions include Mean Squared Error for regression tasks and Cross-Entropy Loss for classification tasks. The choice of loss function affects the gradient and, consequently, the direction and magnitude of weight updates.</p>
<h2>Practical Considerations</h2>
<ul>
<li><strong>Convergence</strong>: SGD may exhibit noisy convergence due to its stochastic nature. However, this noise can help the algorithm escape local minima.</li>
<li><strong>Efficiency</strong>: SGD is computationally efficient because it processes one example at a time, which is particularly advantageous with large datasets.</li>
<li><strong>Variants</strong>: There are variants of SGD, such as SGD with momentum, which adds a fraction of the previous update to the current one to help accelerate convergence in relevant directions.</li>
</ul>
<h2>Application in Neural Networks</h2>
<p>In neural networks, SGD is applied iteratively over the entire dataset (an epoch). Each iteration involves:</p>
<ol>
<li>Forward pass: Calculating the predicted output.</li>
<li>Backward pass: Calculating the gradient of the loss with respect to the weights.</li>
<li>Update: Adjusting the weights using the SGD update rule.</li>
</ol>
<p>This process continues until the model converges to a satisfactory level of performance or a predetermined number of epochs is reached.</p>
<p>Would you like to know more about how SGD is implemented in specific neural network architectures or about other optimization algorithms?</p>
<p>ResponseID:09d82c67-8a32-46</p>
</div><div title="Minibatch Gradient Descent" id="childContent2" onclick="setVisibileChild('childContent2')" hidden=true><h3>Mini-batch Gradient Descent</h3>
<p>Mini-batch gradient descent is an optimization algorithm used in training neural networks, which strikes a balance between the efficiency of batch gradient descent and the noise of stochastic gradient descent (SGD). Here is a detailed breakdown of its components and implementation:</p>
<h4>Update Rule</h4>
<p>The update rule for mini-batch gradient descent is given by:</p>
<p>[ w_{new} = w_{old} - \eta \cdot \frac{1}{m} \sum_{i=1}^{m} 
abla J(w_{old}, x_i, y_i) ]</p>
<p>Where:</p>
<ul>
<li>( w_{new} ) represents the updated weight after processing a mini-batch.</li>
<li>( w_{old} ) is the weight before the update.</li>
<li>( \eta ) is the learning rate, which determines the step size during each update.</li>
<li>( m ) is the size of the mini-batch, typically ranging from 32 to 256, though this can vary based on the dataset and computational resources.</li>
<li>( 
abla J(w_{old}, x_i, y_i) ) denotes the gradient of the loss function ( J ) with respect to the weights, calculated for each example ( (x_i, y_i) ) in the mini-batch.</li>
</ul>
<h4>Advantages</h4>
<ol>
<li><strong>Computational Efficiency</strong>: By processing data in mini-batches, mini-batch gradient descent can utilize parallel processing capabilities of modern hardware, such as GPUs, more effectively than SGD.</li>
<li><strong>Reduced Variance</strong>: Compared to SGD, mini-batch gradient descent has less noisy updates because the gradient is averaged over multiple examples, leading to more stable convergence.</li>
<li><strong>Memory Usage</strong>: It requires less memory than batch gradient descent since it processes only a subset of the data at a time, making it feasible for larger datasets.</li>
</ol>
<h4>Implementation Details</h4>
<ul>
<li><strong>Batch Selection</strong>: Mini-batches are typically selected randomly from the training data to ensure that the model sees a diverse set of examples in each iteration.</li>
<li><strong>Epochs and Iterations</strong>: An epoch is completed when all training examples have been used for updating the model. Within an epoch, multiple iterations occur, each processing a different mini-batch.</li>
<li><strong>Learning Rate Scheduling</strong>: The learning rate ( \eta ) may be adjusted during training using techniques like learning rate decay or adaptive methods such as Adam or RMSprop to improve convergence.</li>
</ul>
<h4>Challenges</h4>
<ul>
<li><strong>Choosing Mini-batch Size</strong>: The choice of ( m ) can significantly impact the training dynamics. A smaller ( m ) can lead to faster but noisier updates, while a larger ( m ) can result in slower but more stable updates.</li>
<li><strong>Convergence</strong>: The balance between speed and stability in mini-batch gradient descent requires careful tuning of hyperparameters, including the learning rate and mini-batch size.</li>
</ul>
<h4>Practical Considerations</h4>
<p>In practice, mini-batch gradient descent is widely used due to its effectiveness in training deep neural networks. It is implemented in many deep learning frameworks like TensorFlow and PyTorch, where users can easily specify the mini-batch size and other training parameters.</p>
<p>Would you like to know more about how to tune the mini-batch size or about specific applications of mini-batch gradient descent in different neural network architectures?</p>
<p>ResponseID:c641003e-c502-4a</p>
</div>
    </div>
  </div>

    <script>
        function saveButton() {
          const content = document.documentElement.outerHTML; // Get the entire HTML content
          const blob = new Blob([content], { type: 'text/html' }); // Create a Blob from the content
          const url = URL.createObjectURL(blob); // Create a URL for the Blob
          const divInnerText = document.getElementById('content').innerText;
          let filename = divInnerText.substring(0, 25);
          const a = document.createElement('a'); // Create an anchor element
          a.href = url; // Set the href to the Blob URL
          
        a.download =  filename + '.html'; // Set the download attribute with a filename
          document.body.appendChild(a); // Append the anchor to the body
          a.click(); // Programmatically click the anchor to trigger the download
          document.body.removeChild(a); // Remove the anchor from the document
          URL.revokeObjectURL(url); // Release the Blob URL
        }
    </script>

    <script>
      function setChildSubjects(){
        let dropDownOptions = [];
        let childSubjects = document.getElementById('childDivs');
        for (let child of childSubjects.children){
           let subject = child.title;
           let optionValue = child.id;

           dropDownOptions.push({subject: subject, value: optionValue});
        
        }

        for (let option of dropDownOptions){
          let optionElement = document.createElement('option');
          optionElement.value = option.value;
          optionElement.text = option.subject;
          document.getElementById('childSubjects').appendChild(optionElement);
        }

        document.getElementById('childSubjects').addEventListener('change', function() {
          if(this.value == "home"){
            goHomePage();
          }else{
            setVisibleChild(this.value);
          }

          }
        );
      }
    </script>


    <script>
      function setVisibleChild(id){
        //quick flicker home to reset state, this allows hoping between child views
     
    
           let topNode = document.getElementById('content');
           let parentNode = document.getElementById('parentContent');
           let childVisibleNode = document.getElementById(id);
           let childDivs = document.getElementById('childDivs');
           for (let child of childDivs.children){
            if(child.id != id){
              child.hidden = true;
            }else{
              child.hidden = false; //redudant
            }
           }
           parentNode.hidden = true;
           childVisibleNode.hidden = false;
         }
      
    </script>

    <script>
      function goHomePage(){
         let topNode = document.getElementById('content');
         let parentNode = document.getElementById('parentContent');
         let children = document.getElementById('childDivs');
         for (let child of children.children){
            child.hidden = true;
         }
         parentNode.hidden = false;
      }
    </script>

    <script>
      function setChildLinks(){
        let children = document.getElementById('childDivs'); 
        let childSubjects = [];
        let discoveredMatches = [];

        
        for (let child of children.children){
          let subject = child.title;
          childSubjects.push({subject: subject, child: child});
          }
             
          let parentNodeH2Subjects = []
          let parentNodeH3Subjects = []
          let parentNodeH4Subjects = []
          let parentNode = document.getElementById('parentContent');
          let H2s = parentNode.getElementsByTagName("H2");
          let H3s = parentNode.getElementsByTagName("H3");
          let H4s = parentNode.getElementsByTagName("H4");
          let isH2Match = false;
          let isH3Match = false;
          let isH4Match = false;

          H2s.length > 0? parentNodeH2Subjects = H2s.map(item => ({subject: item.innerText, item: item})):isH2Match = false;
          H3s.length > 0?parentNodeH3Subjects = H3s.map(item => ({subject: item.innerText, item: item})):isH3Match = false;
          H4s.length > 0?parentNodeH4Subjects = H4s.map(item => ({subject: item.innerText, item: item})):isH4Match = false;

          
           isH3Match = H3s.length == childSubjects.length;
           isH2Match = H2s.length == childSubjects.length;
           isH4Match = H4s.length == childSubjects.length;
          let allDiscovered = false;

          if (isH3Match){
            //We have a match.  We need to find the H2s
            allDiscovered = true;
          }
          if (isH2Match){
            //We have a match.  We need to find the H3s
            allDiscovered = true;
          }
          if (isH4Match){
            //We have a match.  We need to find the H2s
            allDiscovered = true;
          }

          if(allDiscovered && (isH3Match + isH2Match + isH4Match) > 1){
            //wierd.  ok we work from scratch.
            allDiscovered = false;
          }

          if(!allDiscovered){

            let fluffWords = ["a", "an", "the", "and", "but", "or", "for", "nor", "on", "at", "to", "from", "by", "with", "of"];
            let fluffWordsRegex = new RegExp(fluffWords.join("|"), "g");
          
          //OK time to stumble through unpredictable llm output
          //First check if the child subjects are in the parent node h2 subjects
          let isH2 = false;
          let isH3 = false;
          let isH4 = false;
      
          //This can be optimized later.  Probably doesnt matter since it is client side with modern computing.
          for (let i = 0; i < parentNodeH2Subjects.length; i++){
            let subject = parentNodeH2Subjects[i].subject;
            for (let j = 0; j < childSubjects.length; j++){
              let childSubject = childSubjects[j].subject;
              if(childSubject.replace(fluffWordsRegex, "") == subject.replace(fluffWordsRegex, "")){
                isH2 = true;
                discoveredMatches.push({parentLink: parentNodeH2Subjects[i].item, childLink: childSubjects[j].child});
              }
            }

          }
         
          if(!isH2){
            for (let i = 0; i < parentNodeH3Subjects.length; i++){
              let subject = parentNodeH3Subjects[i].subject;
              for (let j = 0; j < childSubjects.length; j++){
                let childSubject = childSubjects[j].subject;  
                if(childSubject.replace(fluffWordsRegex, "") == subject.replace(fluffWordsRegex, "")){
                  isH3 = true;
                  discoveredMatches.push({parentLink: parentNodeH3Subjects[i].item, childLink: childSubjects[j].child});
                }
              }
            }
          }

          if(!isH3){
            for (let i = 0; i < parentNodeH4Subjects.length; i++){
              let subject = parentNodeH4Subjects[i].subject;
              for (let j = 0; j < childSubjects.length; j++){
                let childSubject = childSubjects[j].subject;
                if(childSubject.replace(fluffWordsRegex, "") == subject.replace(fluffWordsRegex, "")){
                  isH4 = true;
                  discoveredMatches.push({parentLink: parentNodeH4Subjects[i].item, childLink: childSubjects[j].child});
                }
              }
            }
          }

          if(!isH4){
              //I suppose there are edge cases but this surely covers 99.9% of the cases.
          }

          }else{
            if(isH3Match){
              for (let i = 0; i < parentNodeH3Subjects.length; i++){
                let child = childSubjects[i].child;
                makeChildLinks(parentNodeH3Subjects[i].item, child);
              }
              return; //bye bye
            }
            if(isH2Match){
              for (let i = 0; i < parentNodeH2Subjects.length; i++){
                let child = childSubjects[i].child;
                makeChildLinks(parentNodeH2Subjects[i].item, child);
                return; //bye bye
              }
              if(isH4Match){
                for (let i = 0; i < parentNodeH4Subjects.length; i++){
                  let child = childSubjects[i].child;
                  makeChildLinks(parentNodeH4Subjects[i].item, child);
                }
              }

            }

          }

          for (let match of discoveredMatches){
            makeChildLinks(match.parentLink, match.childLink);
            if(match.length < childSubjects.length){ alert("LLMM added additional info that is hidden");}
          }

        }
    </script>

    <script>
       function makeChildLinks(parent, child){
            parent.style.cursor = "pointer";
            parent.style.textDecoration = "underline";
            parent.style.color = "blue";
            parent.onclick = function(){
              setVisibleChild(child.id);
            }
       }

    </script>

<script>window.onload = setChildSubjects;</script>

</body>
</html>


   
    <!-- <textarea id="userPrompt"></textarea>

    <script>
        function nextButton() {
            const userPrompt = document.getElementById('userPrompt').value;
            console.log(userPrompt);
        }
    </script> -->