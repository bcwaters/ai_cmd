<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>Home Page</title>
</head>
<body>
  <nav>
    <button id="saveButton" onclick="saveButton()">Save html</button>
    <a id="parentContent" href='javascript:goHomePage()'>Home</a>
    <select id="childSubjects">
      <option value="home">Home</option>
      <!-- Children are added here onload with setChildSubjects -->
    </select>
    <!-- Add a previous and next button for prior requests? -->
    <!-- Add a button to save the current page as a markdown file -->
  </nav>

  <div id="content">  
    <div id="parentContent">
        <h1>LLM Architectures</h1>
<p>Here are three different architectures commonly used in Large Language Models (LLMs):</p>
<h2>1. <strong>Transformer Architecture</strong></h2>
<p>The Transformer architecture, introduced in the paper "Attention is All You Need" by Vaswani et al., has become the foundation for many modern LLMs. Key features include:</p>
<ul>
<li><strong>Self-Attention Mechanism</strong>: Allows the model to weigh the importance of different words in the input sequence.</li>
<li><strong>Encoder-Decoder Structure</strong>: Typically used in tasks like translation, where the encoder processes the input and the decoder generates the output.</li>
<li><strong>Parallel Processing</strong>: Enables efficient training on large datasets due to its ability to process input sequences in parallel.</li>
</ul>
<p>Example models: BERT, GPT, T5.</p>
<h2>2. <strong>Recurrent Neural Networks (RNN) Architecture</strong></h2>
<p>RNNs were one of the earliest architectures used for sequence modeling. Key features include:</p>
<ul>
<li><strong>Sequential Processing</strong>: Processes input data sequentially, maintaining an internal state (memory) that captures information about previous inputs.</li>
<li><strong>Variants</strong>: Includes Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), which address the vanishing gradient problem in traditional RNNs.</li>
<li><strong>Applications</strong>: Suitable for tasks like time series prediction, speech recognition, and text generation.</li>
</ul>
<p>Example models: Simple RNN, LSTM, GRU.</p>
<h2>3. <strong>Sparse Transformer Architecture</strong></h2>
<p>Sparse Transformers aim to improve the efficiency of traditional Transformers by reducing computational complexity. Key features include:</p>
<ul>
<li><strong>Sparse Attention Patterns</strong>: Instead of attending to all positions in the input sequence, the model attends to a subset, reducing the computational load.</li>
<li><strong>Scalability</strong>: Allows for processing longer sequences and larger models with less memory and computational resources.</li>
<li><strong>Applications</strong>: Useful in scenarios where efficiency is critical, such as real-time processing or handling very large datasets.</li>
</ul>
<p>Example models: Sparse Transformer, Reformer, Longformer.</p>

    </div>
    <div id="childDivs">
      <div title=""Transformer Architecture" id="childContent1" onclick="setVisibileChild('childContent1')" hidden=true><h1>Transformer Architecture</h1>
<p>The Transformer architecture represents a significant advancement in the field of natural language processing and sequence-to-sequence tasks. Introduced in the paper "Attention Is All You Need" by Vaswani et al. in 2017, it has since become a foundational model for numerous applications, including machine translation, text summarization, and language understanding.</p>
<h3>Key Components</h3>
<h4><strong>Self-Attention Mechanism</strong></h4>
<p>At the core of the Transformer is the self-attention mechanism, which allows the model to weigh the importance of different words in a sentence relative to each other. This mechanism enables the model to capture contextual relationships between words, regardless of their positions in the input sequence. The self-attention is calculated as follows:</p>
<p>[ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V ]</p>
<p>Where:</p>
<ul>
<li>( Q ) (Query), ( K ) (Key), and ( V ) (Value) are derived from the input embeddings.</li>
<li>( d_k ) is the dimension of the keys.</li>
</ul>
<p>This formula allows the model to focus on different parts of the input sequence simultaneously, which is particularly useful for understanding long-range dependencies.</p>
<h4><strong>Multi-Head Attention</strong></h4>
<p>The Transformer uses multi-head attention to enhance its ability to focus on different aspects of the input. By running several attention mechanisms in parallel, the model can jointly attend to information from different representation subspaces at different positions. This is achieved by linearly projecting the queries, keys, and values ( h ) times with different, learned linear projections to ( d_k ), ( d_k ), and ( d_v ) dimensions, respectively. The outputs are concatenated and once again projected, resulting in the final values.</p>
<h4><strong>Positional Encoding</strong></h4>
<p>Since the Transformer lacks recurrence and convolutions, it must somehow account for the order of the sequence. Positional encodings are added to the input embeddings at the bottom of the encoder and decoder stacks. These encodings can be learned or fixed, but the original paper used sine and cosine functions of different frequencies:</p>
<p>[ PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) ]
[ PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) ]</p>
<h4><strong>Encoder and Decoder Stacks</strong></h4>
<p>The Transformer consists of an encoder and a decoder, each composed of multiple identical layers. The encoder processes the input sequence and produces a continuous representation that the decoder can use to generate the output sequence. Both the encoder and decoder layers contain a multi-head self-attention mechanism, followed by a position-wise fully connected feed-forward network.</p>
<h3>Applications</h3>
<p>Transformers have been successfully applied in numerous areas:</p>
<ul>
<li><strong>Machine Translation</strong>: Models like Google's BERT and OpenAI's GPT series leverage Transformer architectures to achieve state-of-the-art results in translating text between languages.</li>
<li><strong>Text Summarization</strong>: Transformers can generate concise summaries of longer documents by understanding the key points and their relationships.</li>
<li><strong>Language Understanding</strong>: By training on large datasets, Transformers can understand and generate human-like text, making them valuable in chatbots and virtual assistants.</li>
</ul>
<h3>Advantages</h3>
<ul>
<li><strong>Parallelization</strong>: Unlike recurrent models, Transformers can process all input tokens simultaneously, leading to significant speedups in training and inference.</li>
<li><strong>Long-range Dependencies</strong>: The self-attention mechanism allows Transformers to capture dependencies between distant tokens effectively.</li>
</ul>
<h3>Challenges</h3>
<ul>
<li><strong>Computational Complexity</strong>: The attention mechanism's quadratic complexity with respect to the sequence length can be a bottleneck for very long sequences.</li>
<li><strong>Memory Usage</strong>: The need to store all attention weights can lead to high memory consumption, especially for large models.</li>
</ul>
<p>In summary, the Transformer architecture has revolutionized the way we approach sequence-to-sequence tasks, offering a highly effective method for capturing dependencies and understanding context in natural language processing and beyond.</p>
<p>ResponseID:263ba4cf</p>
</div><div title=" Recurrent Neural Networks" id="childContent2" onclick="setVisibileChild('childContent2')" hidden=true><h1>Recurrent Neural Networks</h1>
<p>Recurrent Neural Networks (RNNs) are a class of artificial neural networks designed to recognize patterns in sequences of data, such as text, genomes, handwriting, or spoken words. They are particularly suited for tasks where the context and order of the input data are important, making them ideal for applications in natural language processing, speech recognition, and time-series prediction.</p>
<h3>Key Features</h3>
<h4>Sequential Processing</h4>
<p>RNNs process input sequences one element at a time, maintaining an internal state that captures information about what has been processed so far. This internal state or "memory" allows RNNs to use previous inputs to influence the processing of current and future inputs, which is crucial for understanding context and dependencies in sequential data.</p>
<h4>Handling Variable Length Input</h4>
<p>One of the strengths of RNNs is their ability to handle inputs of varying lengths. This is particularly useful in natural language processing, where sentences or paragraphs can be of different lengths. The network processes each element in the sequence, adjusting its internal state dynamically as it moves through the sequence.</p>
<h4>Backpropagation Through Time (BPTT)</h4>
<p>Training RNNs involves a specialized form of backpropagation known as Backpropagation Through Time (BPTT). In BPTT, the gradient of the loss function is calculated with respect to the weights of the network over multiple time steps. This allows the network to learn long-term dependencies, though it can also lead to challenges like the vanishing or exploding gradient problem.</p>
<h3>Applications</h3>
<h4>Natural Language Processing (NLP)</h4>
<p>RNNs are widely used in NLP tasks such as language translation, sentiment analysis, and text generation. Their ability to maintain context over long sequences makes them effective for understanding and generating human language.</p>
<h4>Speech Recognition</h4>
<p>In speech recognition, RNNs can model the temporal dynamics of speech, allowing them to convert spoken words into text with high accuracy. They are particularly effective in handling the variability and nuances of human speech.</p>
<h4>Time-Series Prediction</h4>
<p>RNNs are also used in predicting future values in time-series data, such as stock prices, weather forecasts, and traffic flow. Their ability to capture temporal dependencies makes them suitable for these types of tasks.</p>
<h3>Challenges and Limitations</h3>
<h4>Vanishing and Exploding Gradients</h4>
<p>One of the main challenges in training RNNs is the vanishing and exploding gradient problem. This occurs when gradients become too small or too large during backpropagation, making it difficult for the network to learn long-term dependencies. Techniques like Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) have been developed to mitigate these issues.</p>
<h4>Computational Complexity</h4>
<p>RNNs can be computationally intensive, especially for long sequences. Each time step requires processing the entire input up to that point, which can lead to increased computational costs and memory usage.</p>
<h3>Example Models</h3>
<h4>Long Short-Term Memory (LSTM)</h4>
<p>LSTMs are an extension of RNNs that address the vanishing gradient problem by incorporating a gating mechanism. This allows them to selectively remember or forget information over long sequences, making them particularly effective for tasks requiring long-term memory.</p>
<h4>Gated Recurrent Units (GRU)</h4>
<p>GRUs are another variant of RNNs designed to simplify the LSTM architecture while maintaining similar performance. They use fewer parameters and are often faster to train, making them a popular choice in many applications.</p>
<p>In summary, Recurrent Neural Networks are a powerful tool for processing sequential data, with applications across various fields. However, they come with their own set of challenges that require careful consideration and often the use of specialized architectures like LSTMs and GRUs to overcome.</p>
<p>ResponseID:fa7db7c2</p>
</div><div title=" Sparse Transformer Architecture"" id="childContent3" onclick="setVisibileChild('childContent3')" hidden=true><h1>Sparse Transformer Architecture</h1>
<p>The Sparse Transformer architecture is an advancement over the traditional Transformer architecture, designed to enhance efficiency and scalability. Here's a more detailed look at the features and applications of the Sparse Transformer architecture:</p>
<h3>Key Features</h3>
<h4>Sparse Attention Patterns</h4>
<ul>
<li>Unlike traditional Transformers that attend to all positions in the input sequence, Sparse Transformers utilize sparse attention patterns. This means the model attends to a subset of positions rather than the entire sequence, which significantly reduces computational load.</li>
<li>The sparse attention mechanism allows the model to focus on relevant parts of the input, making it more efficient in handling longer sequences and larger datasets.</li>
</ul>
<h4>Scalability and Efficiency</h4>
<ul>
<li>By reducing the computational complexity, Sparse Transformers can process larger sequences and larger models with less memory and computational resources. This scalability is crucial for applications that require handling vast amounts of data.</li>
<li>The efficiency gained from sparse attention also enables faster processing times, which is beneficial in scenarios where real-time processing is required.</li>
</ul>
<h3>Applications</h3>
<h4>Real-Time Processing</h4>
<ul>
<li>Sparse Transformers are particularly useful in scenarios where efficiency is critical, such as real-time processing. The reduced computational overhead allows for quicker responses in applications like real-time language translation or interactive systems.</li>
</ul>
<h4>Handling Very Large Datasets</h4>
<ul>
<li>The ability to process larger datasets with fewer resources makes Sparse Transformers ideal for tasks involving big data. This includes training on extensive corpora for language models or processing large-scale datasets in other domains like genomics or climate modeling.</li>
</ul>
<h3>Example Models</h3>
<ul>
<li><strong>Sparse Transformer</strong>: The original model that introduced the concept of sparse attention, demonstrating significant improvements in efficiency over traditional Transformers.</li>
<li><strong>Reformer</strong>: An extension of the Sparse Transformer, which further optimizes memory usage and processing speed, making it suitable for even larger models and datasets.</li>
<li><strong>Longformer</strong>: Another variant that applies sparse attention to enable the processing of very long sequences, useful in tasks like document summarization and long text analysis.</li>
</ul>
<p>In summary, the Sparse Transformer architecture represents a significant step forward in the design of large language models, offering enhanced efficiency and scalability that can be leveraged in a wide range of applications.</p>
<p>ResponseID:d9eea0be</p>
</div>
    </div>
  </div>

    <script>
        function saveButton() {
          const content = document.documentElement.outerHTML; // Get the entire HTML content
          const blob = new Blob([content], { type: 'text/html' }); // Create a Blob from the content
          const url = URL.createObjectURL(blob); // Create a URL for the Blob
          const divInnerText = document.getElementById('content').innerText;
          let filename = divInnerText.substring(0, 25);
          const a = document.createElement('a'); // Create an anchor element
          a.href = url; // Set the href to the Blob URL
          
        a.download =  filename + '.html'; // Set the download attribute with a filename
          document.body.appendChild(a); // Append the anchor to the body
          a.click(); // Programmatically click the anchor to trigger the download
          document.body.removeChild(a); // Remove the anchor from the document
          URL.revokeObjectURL(url); // Release the Blob URL
        }
    </script>

    <script>
      function setChildSubjects(){
        let dropDownOptions = [];
        let childSubjects = document.getElementById('childDivs');
        for (let child of childSubjects.children){
           let subject = child.title;
           let optionValue = child.id;

           dropDownOptions.push({subject: subject, value: optionValue});
        
        }

        for (let option of dropDownOptions){
          let optionElement = document.createElement('option');
          optionElement.value = option.value;
          optionElement.text = option.subject;
          document.getElementById('childSubjects').appendChild(optionElement);
        }

        document.getElementById('childSubjects').addEventListener('change', function() {
          if(this.value == "home"){
            goHomePage();
          }else{
            setVisibleChild(this.value);
          }

          }
        );
      }
    </script>


    <script>
      function setVisibleChild(id){
        //quick flicker home to reset state, this allows hoping between child views
     
         if(id != "parentContent"){
           let topNode = document.getElementById('content');
           let parentNode = document.getElementById('parentContent');
           let childVisibleNode = document.getElementById(id);
           let childDivs = document.getElementById('childDivs');
           for (let child of childDivs.children){
            if(child.id != id){
              child.hidden = true;
            }else{
              child.hidden = false; //redudant
            }
           }
           parentNode.hidden = true;
           childVisibleNode.hidden = false;
         }
      }
    </script>

    <script>
      function goHomePage(){
         let topNode = document.getElementById('content');
         let parentNode = document.getElementById('parentContent');
         let children = document.getElementById('childDivs');
         for (let child of children.children){
            child.hidden = true;
         }
         parentNode.hidden = false;
      }
    </script>

    <script>
      function setChildLinks(){
        let children = document.getElementById('childDivs'); 
        let childSubjects = [];
        let discoveredMatches = [];

        
        for (let child of children.children){
          let subject = child.title;
          childSubjects.push({subject: subject, child: child});
          }
             
          let parentNodeH2Subjects = []
          let parentNodeH3Subjects = []
          let parentNodeH4Subjects = []
          let parentNode = document.getElementById('parentContent');
          let H2s = parentNode.getElementsByTagName("H2");
          let H3s = parentNode.getElementsByTagName("H3");
          let H4s = parentNode.getElementsByTagName("H4");
          let isH2Match = false;
          let isH3Match = false;
          let isH4Match = false;

          H2s.length > 0? parentNodeH2Subjects = H2s.map(item => ({subject: item.innerText, item: item})):isH2Match = false;
          H3s.length > 0?parentNodeH3Subjects = H3s.map(item => ({subject: item.innerText, item: item})):isH3Match = false;
          H4s.length > 0?parentNodeH4Subjects = H4s.map(item => ({subject: item.innerText, item: item})):isH4Match = false;

          
           isH3Match = H3s.length == childSubjects.length;
           isH2Match = H2s.length == childSubjects.length;
           isH4Match = H4s.length == childSubjects.length;
          let allDiscovered = false;

          if (isH3Match){
            //We have a match.  We need to find the H2s
            allDiscovered = true;
          }
          if (isH2Match){
            //We have a match.  We need to find the H3s
            allDiscovered = true;
          }
          if (isH4Match){
            //We have a match.  We need to find the H2s
            allDiscovered = true;
          }

          if(allDiscovered && (isH3Match + isH2Match + isH4Match) > 1){
            //wierd.  ok we work from scratch.
            allDiscovered = false;
          }

          if(!allDiscovered){

            let fluffWords = ["a", "an", "the", "and", "but", "or", "for", "nor", "on", "at", "to", "from", "by", "with", "of"];
            let fluffWordsRegex = new RegExp(fluffWords.join("|"), "g");
          
          //OK time to stumble through unpredictable llm output
          //First check if the child subjects are in the parent node h2 subjects
          let isH2 = false;
          let isH3 = false;
          let isH4 = false;
      
          //This can be optimized later.  Probably doesnt matter since it is client side with modern computing.
          for (let i = 0; i < parentNodeH2Subjects.length; i++){
            let subject = parentNodeH2Subjects[i].subject;
            for (let j = 0; j < childSubjects.length; j++){
              let childSubject = childSubjects[j].subject;
              if(childSubject.replace(fluffWordsRegex, "") == subject.replace(fluffWordsRegex, "")){
                isH2 = true;
                discoveredMatches.push({parentLink: parentNodeH2Subjects[i].item, childLink: childSubjects[j].child});
              }
            }

          }
         
          if(!isH2){
            for (let i = 0; i < parentNodeH3Subjects.length; i++){
              let subject = parentNodeH3Subjects[i].subject;
              for (let j = 0; j < childSubjects.length; j++){
                let childSubject = childSubjects[j].subject;  
                if(childSubject.replace(fluffWordsRegex, "") == subject.replace(fluffWordsRegex, "")){
                  isH3 = true;
                  discoveredMatches.push({parentLink: parentNodeH3Subjects[i].item, childLink: childSubjects[j].child});
                }
              }
            }
          }

          if(!isH3){
            for (let i = 0; i < parentNodeH4Subjects.length; i++){
              let subject = parentNodeH4Subjects[i].subject;
              for (let j = 0; j < childSubjects.length; j++){
                let childSubject = childSubjects[j].subject;
                if(childSubject.replace(fluffWordsRegex, "") == subject.replace(fluffWordsRegex, "")){
                  isH4 = true;
                  discoveredMatches.push({parentLink: parentNodeH4Subjects[i].item, childLink: childSubjects[j].child});
                }
              }
            }
          }

          if(!isH4){
              //I suppose there are edge cases but this surely covers 99.9% of the cases.
          }

          }else{
            if(isH3Match){
              for (let i = 0; i < parentNodeH3Subjects.length; i++){
                let child = childSubjects[i].child;
                makeChildLinks(parentNodeH3Subjects[i].item, child);
              }
              return; //bye bye
            }
            if(isH2Match){
              for (let i = 0; i < parentNodeH2Subjects.length; i++){
                let child = childSubjects[i].child;
                makeChildLinks(parentNodeH2Subjects[i].item, child);
                return; //bye bye
              }
              if(isH4Match){
                for (let i = 0; i < parentNodeH4Subjects.length; i++){
                  let child = childSubjects[i].child;
                  makeChildLinks(parentNodeH4Subjects[i].item, child);
                }
              }

            }

          }

          for (let match of discoveredMatches){
            makeChildLinks(match.parentLink, match.childLink);
            if(match.length < childSubjects.length){ alert("LLMM added additional info that is hidden");}
          }

        }
    </script>

    <script>
       function makeChildLinks(parent, child){
            parent.style.cursor = "pointer";
            parent.style.textDecoration = "underline";
            parent.style.color = "blue";
            parent.onclick = function(){
              setVisibleChild(child.id);
            }
       }

    </script>

<script>window.onload = setChildSubjects;</script>

</body>
</html>


   
    <!-- <textarea id="userPrompt"></textarea>

    <script>
        function nextButton() {
            const userPrompt = document.getElementById('userPrompt').value;
            console.log(userPrompt);
        }
    </script> -->