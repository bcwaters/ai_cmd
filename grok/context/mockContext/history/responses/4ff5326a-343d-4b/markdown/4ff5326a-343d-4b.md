Here are some additional LaTeX equations that further illustrate concepts in linear algebra and neural networks:

### Activation Functions

In neural networks, activation functions introduce non-linearity to the model. A common activation function is the sigmoid function, which can be written as:

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

This function squashes the input to a range between 0 and 1.

### Softmax Function

The softmax function is often used in the output layer of a neural network for multi-class classification. It is defined as:

$$
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}
$$

where $z_i$ is the input to the output layer and $K$ is the number of classes.

### Batch Normalization

Batch normalization normalizes the output of a layer to have a mean of zero and a standard deviation of one. The formula for batch normalization is:

$$
\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
$$

where $\mu_B$ and $\sigma_B^2$ are the mean and variance of the mini-batch, and $\epsilon$ is a small constant to avoid division by zero.

### Gradient Descent

Gradient descent is used to minimize the loss function during training. The update rule for weights in gradient descent is:

$$
w_{new} = w_{old} - \alpha \cdot \frac{\partial \mathcal{L}}{\partial w}
$$

where $\alpha$ is the learning rate and $\frac{\partial \mathcal{L}}{\partial w}$ is the gradient of the loss function with respect to the weights.

### Jacobian Matrix

The Jacobian matrix is used in backpropagation to compute the gradient of a vector-valued function. For a function $\mathbf{f}: \mathbb{R}^n \rightarrow \mathbb{R}^m$, the Jacobian matrix $\mathbf{J}$ is:

$$
\mathbf{J} = \begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n}
\end{bmatrix}
$$

These equations provide a deeper understanding of how linear algebra and calculus are applied in the context of neural networks.